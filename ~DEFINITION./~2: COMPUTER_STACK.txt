~FOCUS:
	: BASH SCRIPT FOR COMPUTER STACK: ACCESSING WEB, SCRIPT WRITING
	: Review CRASHCOURSE: Computer Science
	:
~WHY: FOR THE COMPUTE
    : DIFFERENCE ENGINE
    : ANALYTICAL ENGINE
    : FOR THE TABULATING OF THE CENSUS
    : GLOBALIZED TRADE

~HOW: ELECTRONIC COMPUTING
    : HARVARD MK 1: Mechanical Relays
    : Thermaionic Valve >>> Triode Vaccuum Tube
    : THE BOMBE >>> COLOSSUS MK 1
    : IBM 608: Transistors


~MODERN COMPUTER STACK: Transistor - Web Browser
~0: DIVIDE/UNCERTAINTY: Random variable 
	: NOT-IN-NOT-OUT:
		Hide	| Block
		Mute	| Deafen
		Pause	| Detach
		Choke	| Mask
		Break	| Fast
	: Error/Flags     
		Collision/Network Congestion= conflict with transfer in carrier
			Routine Problem, Hop Limit
			Scrambled Packets
			Growing Wait-Time
		Kernel Panic/Crash, Overflow= not enough memory bit 
		Dirty Bit= Mismatch between Cache and RAM, Bugs, Worm, Malware 
    : Cybersecurity:
        : Threat Model, 
            : Attack vector
            : Brute Force Attack
            : 
        : Questions: 
            : Who are you?
            : What should you have access to?
            : What you have?
            : What you are?
        : Authentication
            : Passwords
            : 2/Multi-Factor
        : Premission:
            : Read = Allows a user to see the contents of a file
            : Write = Allows a user to modify the contents
            : Execute = Allows user to run a file
        : Malware:
            : 
        : Security Kernel =
            : Independent Verification & Validation
        : Isolation/Sandbox
        : Virtual Machine
    : Hackers: 
        : White Hats
        : Black Hats
        : Social Engineering
            : Phishing
            : Pre-texting
        : NAND Mirroring 
        : Malware:
            : Trojan Horse
        : Bug Exploits:
            : Buffer Overflow
            : Bounds Checking
            : Canaries
            : Code Injections
        : Zero Day Vulnerability
        : Security Patches 
        : DDos Attacks
    : Cryptography: Defense
        : Encription 
            : Symmetric
            : Asymmetric
                : Public
                : Private
        : Decryption
        : Ceasar Cipher
        : Cryptoanalyst
        : Subsctitution Cipher
        : Columnar Transposition Cipher
            : Enigma Rotor
            : The Bombe
        : Data Encryption Standard
            : Advanced Encryption Standard
        : Key Exchange
            : Diffie-Hellman
        : Modular Exponentiation
	
~1: IN/FETCH:SEE/HEAR/FEEL/SMELL/TASTE() ~4: OUT/EXECUTE:BE(RIGHT), HARDWARES:
	: IN-OUTPUT DEVICES/PERIPHERALS:
		: See/Camera      = Show/Screen, GUI, CRTs>Scanning>Pixels, DesktopMetaphor, 3D Projection,
		: Hear/Microphone = Speak/Speaker
		: Feel/Keyb/Mouse = Move/Animation/Events
		: Inhale/Cooler   = Exhale/Exhaust
		: Eat/Power       = Create/Heat

: HARDWARE: Transistorized Computers
    : Computer (Physical) Network:: The signals are received and converted back into data bits.
        : DNS/WEB Server, Data Centers?
        : Router, NetworkAdapter/Chips
        : Switch, CollisionDomain
        : CopperWire/EthernetCable, RadioWaves/WiFi
    : Parallelization > Instruction Pipelining > Pipelined Superscalar > Multi-core CPU's
        Multi-bit Bus: a[n], number of bit, More Bits for more Instruction/Memory Length
    : Central/Graphical Processing Unit (C/GPU): 
        : Clock 
        : Temporary Memory Registers:
            : Instruction Address Register >>> Address Input >>> Address = Data in RandomAccessMemory (RAM) >>> Instruction Register: FETCH PHASE
                : MUX: (a AND NOTsel) OR (sel AND b)
                    : NOT (in=sel, out=NOTsel)
                    : AND (in=a, in=NOTsel, out=aANDNOTsel)
                    : AND (in=sel, in=b, out=selANDb)
                    : OR (in=aANDNOTsel, in=selANDb, out=out)
                : Matrix >>> Gate >>> AND-OR Latch: Memory Address
                    : AND (in=column, in=row, out=columnANDrow)
                        : AND (in=data, in=write-enable, out=set)
                        : NOT (in=data, out=NOTdata)
                        : AND (in=NOTdata, in=write-enable, out=reset)
                            : OR (in=set, in=outLOOP, out=setORoutLoop)
                            : NOT (in=reset, out=NOTreset)
                            : AND (in=setORoutLOOP, in=NOTreset, out=out)
                : Memory Register/Bit: Flip Flops: out[t] = in[t-1]
                    : OR-LOOP (in=a, in=b, out=1)
                    : AND-LOOP (in=a, in=b, out=0) 
            : Instruction Register decodes Data into OperationsCode (OPCODE): DECODE PHASE
                : Load (Read/)
                    : Read 
                : Arithmetic = Half>Full>Multi-bit Adder:
                    : XOR (in=a, in=b, out=abSUM)
                        : AND (in=a, in=b, out=aANDb); 
                        : NOT (in=aANDb, out=NOTaANDb); 
                        : OR (in=a, in=b, out=aORb); 
                        : AND (in=NOTaANDb, in=aORb, out=out);
                    : AND (in=a, in=b, out=abCARRY)	
                        : >XOR (in=abSUM, in=c, out=abcSUM)
                        : >AND (in=abSUM, in=c, out=abcCARRY)
                            ; >OR (in=abCARRY, in=abcBARRY, out=out)
                    : Flags of Bits: Overflow(>), Zero(=), Negative(<)...    
                : Logic: Mux/DMux >>> Xor >>> Or >>> And/Nand >>> Not
    : ControlWire: Transistor>IntegratedCircuit>PrintedCircuitBoards>Semiconductor 

: SOFTWARE:
    : INTERNET PROTOCOLS & LAYERS OF THE OPEN SYSTEN INTERCONNECTION (OSI) MODEL: 
        : Application: The data is handed off to the application that requested it, such as a web browser displaying a webpage.
            : Domain Name System (DNS) + Web-Server
            : Web-Browser/Search Engine: Hypertext Transfer Protocol (HTTP)
                : Web-Address/Page: Universal Resource Locator (URL) + Hyperlinks
                : Hypertext Markup Language (HTML)
                : Index: Frequency of Words + Backlinks
        : Presentation: The data is decoded or decompressed if needed.
        : Session: The session is maintained or closed as necessary.
        : Transport: Segments are reassembled, and any lost data is retransmitted.
            : Transmission Control Protocol (TCP): Sequence Switching/Routing Packets, Acknowledging
        : Network: The packets are reassembled, and IP addresses are used to confirm the data arrived at the correct destination.
            : Internet Protocol (IP) Header: InternetServiceProvider(ISP)>Wide>LocalAreaNetwork>MAC Address
        : Data Link: The frames are reassembled and validated for any transmission errors.
            : Data Payload (UDP): 
                : TCP Header: Software Port + Checksum
                : Data 
    : OPERATING SYSTEM: Fetch >>> Decode >>> Execute, Automation/ Learning, Optimization/Multi-Tasking, Device Driver
        : OPERATION CODE (OPCODE) = INSTRUCTION + MEMORY ADDRESS:
            : LOAD + MEMORY ADDRESS
                : Read (Enable) DATA in MEMORY ADDRESS (RAM)
                : Write (Enable) DATA in Temporary Memory Register
                : 
            : ADD/SUBTRACT + MEMORY ADDRESS(ES)
            : STORE
                : Write (Enable) DATA in MEMORY ADDRESS 
                : Read (Enable) DATA in Temporary Memory Register
            : JUMP / JUMP-NEGATIVE + MEMORY ADDRESS
            : HALT
        : MEMORY: Read-Write, Management/Data Structure: Allocation/Virtualization, Protection, De+Fragmentation
            : Read-Only Memory (ROM) & Access Control: Defines visibility and accessibility of members. To enforce encapsulation and protect data.
            : Class Definitions: Encapsulates data and behavior in a blueprint for objects. To create reusable structures for organizing data and logic.
                : Functions: Encapsulates reusable blocks of code. To simplify and modularize tasks.
            : Transactions: Groups operations as a single unit of work. To ensure data consistency and integrity.
                : Views: Creates virtual tables based on SQL queries. To simplify complex queries and enhance security.
                : Stored Procedures: Encapsulates SQL statements for reuse. To standardize and simplify repetitive tasks.
            : Database Creation: Creates a new database. To systematically organize and store application data.
                : Table Creation: Defines a structured format for storing data in rows and columns. To organize data with specific attributes.
            : File System: Directory = Name, Type, Root>Sub, Hierarchical/Flat, Metadata: Length, 
                : FILE TYPES: Array, Libraries, Node/Tree, Graph/Web/Forest, 3D Matrix: A fixed-size collection of items of the same type stored in contiguous memory. To efficiently store and access multiple values in a single variable.
                    : Wave/Audio: Amplitude, Spectogram
                    : Bitmap: GraphicsGenerator>ScreenBuffer(ImageWidthxHeight)
                    : Character/Text/List/String = ASCII>UNICODE
                    : Number/Integer/Float= 
                        : Scientific Notation; Negative Bit= 2^N-x
                        : Null Value			
            : Pointer: Next Address Reference
                : Primary Key Constraint: Ensures unique identification for each table record. To prevent duplicate records and establish a reliable reference point.
                : Foreign Key Constraint: Links two tables to maintain referential integrity. To model relationships between tables.
            : Indexes(References): Improves the speed of data retrieval. To optimize query performance.
        
        : OPERATION: Function>Algorithm/Sort>Compression: Brute Force, Selection, Merge, Dijkstra... 
            : Console Input/Output: Reads from and writes to the console. To interact with the user.
            : Search/Queries: a sequence of actions that leads from initial state to a goal state
                : Data Querying: Retrieves specific data from the database. To access and analyze stored information.
                : Sub-Query: Uses the result of one query within another query. To filter or derive conditions dynamically.
                : GROUP BY/HAVING: Groups data and filters results based on aggregate conditions. To summarize and analyze grouped data.
                : Joins: Combines rows from multiple tables. To retrieve related data from different tables.
                : Unions: Merges result sets of multiple SELECT queries. To consolidate data into one result set.
            : Initialization: Assignment Statement: A = 1
                : Import/Include: Brings in external libraries or namespaces. To use classes and methods from other libraries.
                : Declarations: Allocates memory space for storing data. To store data that can be used and manipulated in a program.
                : Assignments: Initializes or updates the value of a variable. To give variables usable values.
                : Comparison: Less Than, Greater Than, Equal
            : List: Queue/Stack, Create-Read-Update-Delete, Normalize
                : String and Date Functions: Manipulates string and date values. To format and extract meaningful information.
                : Data Insertion: Adds new records to a table. To populate the database.
                : Data Update: Modifies existing records in a table. To correct or update information.
                : Data Deletion: Removes records from a table. To delete obsolete or incorrect data.
            : Logic: NOT, AND, OR
            : Arithmetic: XOR+ANDCarry/Half, Full, Multi-bit Adder
                : Aggregate Functions: Performs calculations like sum, average, and count. To derive summary statistics.
            : ControlFlow: Conditional Statement: If/While/For... function(input1, 2)
                : Triggers: Automatically executes actions in response to table events. To enforce rules and maintain data integrity.
                : Conditionals: Enables decision-making in code. To perform actions based on conditions.
                    : If/Else If/Else: Executes code based on logical conditions. To handle multiple outcomes.
                    : Switch: Simplifies multi-condition branching. To streamline complex conditional logic.
                    : Exception Handling: Manages runtime errors. To handle errors gracefully and ensure program stability.
                : Loops: Repeats code execution based on conditions. To perform repetitive tasks.
                    : While: Repeats while a condition is true. To handle indefinite iteration.
                    : Do-While: Executes at least once before checking conditions. To ensure at least one iteration.
                    : For: Iterates a specific number of times. To handle definite iteration.
                    : Continue Statement: Skips to the next iteration. To bypass unnecessary code in specific cases.
                    : Break Statement: Exits a loop or switch statement early. To terminate iterations based on conditions.	
            : Execute: Return Statement: Then/Else/Next... 
                : Return: Exits a function and optionally returns a value. To end execution and provide a result.
        : MACHINE CODE:
            : Memory Size/Length: (4/8(byte)/16(word)/32/64/128)
            : Memory Bit: BinaryStates: 0, 1 (Base-2 Notation)

: DEVELOPMENT LIFECYCLE:
    : Planning + Design: Survey-Prove Investors
    : Prototyping (Testing) Development: FAIL FAST!
    : Deployment: Proven Product
    : Maintenance/Warranty
: Engineering Collaboration:
    : Source Control
        : Quality Assurance
        ; Roll Back
    : Repository:
        : Check in vs out
        : Committing

: NOTES: ARTIFICIAL INTELLIGENCE NOTES
    : EXPLORED-SET = KNOWN/FAMILY/HOME/SCHEDULE (CORRECTNESS)
        : sentence = an assertion about the world in a knowledge representation language; Propositional Logic
        : model = assignment of a truth value to every propositional symbol (a "possible world") 
            : Bayesian network = data structure that represents the dependencies among random variables
                : directed graph
                : each node represents a random variable
                : arrow from X to Y means X is a parent of Y
                : each node X has probability distribution P(X|Parents(X))
        : knowledge base = a set of sentences known by a knowledge-based agent
            : Entailment: α ⊨ β = in every model in which sentence α is true, sentence β is also true.
        : clause = a disjunction of literals 
        : conjunctive normal form = logical sentence that is the conjunction of clauses
    
    
    : AGENT = BODY/I/HERE/NOW
        : knowledge-based agent(s) = agents that reason by operating on internal representations of knowledge
    
    : FRONTIER = WHAT/WHO/WHERE/WHEN (UNKNOWN/STRANGER/FOREIGN/SURPRISE)
        : Uncertainty over Time:
        : loss function = function that expresses how poorly our hypothesis performs
            : 0-1 loss function: L(actual, predicted) = 
                : 0 if actual = predicted,
                : 1 otherwise
            : L1 loss function: L(actual, predicted) = | actual - predicted |
            : L2 loss function: L(actual, predicted) = (actual - predicted)
    
       
        : Theorem Proving:
            : initial state: starting knowledge base
            : actions: inference rules
            : transition model: new knowledge base after inference
            : goal test: check statement we're trying to provide
            : path cost function: number of steps in proof
        : Probability: 0 < P(w) < 1
            : unconditional = degree of belief in a proposition in the absence of any other evidence
            : conditional: P(a | b) = degree of belief in a proposition given some evidence that has already been revealed
            : joint = 
        : random variable = a variable in probability theory with a domain of posiible values it can take on
        : probability distribution = list of probabilities
        : independence: P(a and b) = P(a)P(b) = the knowledge that one event occures does not affect the probability of the other event
        : Inference:
            : QUERY X: variable for which to compute distribution
            : Evidence varables E: observed variables for event e
            : Hidden variables Y: non-evidence, non-query variable.
            : Goal: Calculate P(X|e)
        : Markov assumption = the assumption that the current state depends on only a finite fixed number of previous states
            : Markov chain = a sequence of random variables where the distribution of each variable follows the Markov assumption
            : Transition Model
            : Hidden Markov Model = a Markov model for a system with hidden states that generate some observed event
            : Sensor Model
        : sensor Markov assumption = the assumption that the evidence variable deepends only the corresponding state
        : Traveling MailMan Problem 
        
        : Constraint Satisfaction Problem (CSP):
            : variables
            : domains
            : constraints:
                : hard = constraints that must be satisfied in a correct solution
                : soft = constraints that express some notion of which solutions are preferred over others
                : unary = 
                : binary
            : node consistent = when all the values in a variable's domain satisfy the variable's unary constraints
            : CSP as Search Problem:
                : initial state: empty assignment (no variables)
                : actions: add a {variable = value} to assignment
                : transition model: shows how adding an assignment changes the assignment
                : goal test: check if all variables assigned and constraints all satisfied
                : path cost function: all paths have same cost
             : SELECT-UNASSIGNED-VAR 
                : minimum remaining values (MRV) heuristic: select variable that has the smallest domain
                : degree heuristic: select the variable that has the highest degree 
            : DOMAIN-VALUES:
                : least-constraining values heuristic: return variables in order by number of choices that are ruled for neighboring variables 
                    : try least-constraining values first
        : Weight Vector w: (w0, w1, w2)
            : Input Vector x: (1, x1, x2)
        : training set
        : testing set
        : Markov Decision Process = model for decision-making, representing states, actions, and their rewards
            : Set of states S
            : Set of actions ACTIONS(s)
            : Transition model P(s'|s,a)
            : Reward function R(s,a,s')
        : LANGUAGE: 
            : Syntax = structure of sentence
                : Semantics = meaning of sentence
            : formal grammar = a system of rules for generating sentences in a language
                : Context-Free grammar
                    : Sentence -> Noun Phrase + Verb Phrase
            : n-gram = a contiguous sequence of n itams from a sample of text
            : bag-of-words model = model that represents text as an unordered collection of words
            : one-hot representation = representation of meaning as a vector with a sing 1, and with other values as 0
            : distributive representation = representation of meaning distributed across multiple values
            : word2vec = model for generating word vectors



    : GAME:
        : S0:           initial State
        : PLAYER(s):    returns which player to in state s
        : ACTION(s):    returns legal moves in state s
        : RESULT(s, a): returns state after action a taken in state s
        : TERMINAL(s):  checks if sate s is a terminal state
        : UTILITY(s):   final numerical value for terminal state s

~3: READ-WRITE(), CORRECTNESS:
    : SEARCH
        : Uninformed: 
            : Depth-First (Stack)
                : Start with a frontier that contains the intitial state.
                : Start with an empty explored set.
                : Repeat:
                    : If the frontier is empty, then no solution.
                    : Remove a node from the frontier.
                    : If node contains goal state, return the solution.
                    : Add the node to the explored set.
                    : Expand node, add resulting nodes to the frontier, if they are NOT already in the frontier or the explored set.
            : Breadth-First (Queue)
        : Informed: with Heuristic Function
            : Greedy Best-First
            : A*: g(n) + h(n)
                : g(n) = cost to reach node
                : h(n) = estimated cost to goal 
                : optimal if:
                    : h(n) is admissible (never overestimates the true cost)
                    : and h(n) is consistent (for every node n and successor n' with step cost c, h(n) < h(n') + c) 
        : Adversarial: with Adversary
            : Minimax: Win-Lose Goal-State
                : Set Agents:
                    : MAX aims to maximize score
                    : MIN aim to minimize score
                : Set Outcomes: 
                    : -1 = Lose
                    : 0 = Tie
                    : +1 = Win
                : Given a state s:
                    : MAX picks action a in ACTIONS(s) that produces highest value of MIN-VALUE(RESULTS(s, a))
                    : MIN picks action a in ACTION(s) that produces smallest value of MAX-VALUE(RESULTS(s, a))
                : Pseudocode:
                    function MAX-VALUE(state):
                        if TERMINAL(state):
                            return UTILITY(state)
                        v = -infinity
                        for action in ACTIONS(state):
                            v = MAX(v, MIN-VALUE(RESULT(state, action)))
                        return v
                    
                    function MIN-VALUE(state):
                        if TERMINAL(state):
                            return UTILITY(state)
                        v = infinity
                        for action in ACTIONS(state):
                            v = MIN(v, MAX-VALUE(RESULT(state, action)))
                        return v
            : Alpha-Beta Pruning
            : Depth-Limited Minimax: with Evaluation function

    : LOGIC: 
        : Logical Connectives: 
            : not ¬, 
            : and ∧, 
            : or ∨,
            : implication →
            : biconditional ↔
        : Entailment Algorithm: Does KB ⊨ α ?
        : Model Checking: (logic.py)
            : To determine if KB ⊨ α:
                : Enumerate all possible models.
                : If in every model where KB is true, α is true, then KB entals α.
                : Otherwise, KB does not entail α.
        : Knowledge Engineering:
        : Inference Rules:
            : Modus Ponens: α → β
            : And Elimination: α ⊨ β
            : Double Negation Elimination:
            : Implication Elimination
            : Biconditional Elimination
            : De Morgan's Law
            : Distributive Property/Law
            : Unit Resolution
        : Conversion to conjunctive normal form (CNF)
            : Eliminate biconditionals
            : Eliminate implications
            : Move not inwards using De Morgan's laws
            : Use distributive law to distribute or wherever possible
        : Inference by Resolution
            : To determine if KB ⊨ α:
                : Convert (KB and not alpha) to Conjunctive Normal Form.
                : Keep checking to see if we can use resolution to produce a new clause.
                    : If ever we produce the empty clause (equivalent to false), we have a contradiction, and KB ⊨ α.
                    : Otherwise, if we cannot add new clauses, no entailment.
        : First-Order Logic
            : Universal Quantificationa
            : Existential Quantification
    : PROBABILITY: 
        : Bayes' Rule: P(b | a) = P(a | b) P(b) / P(a)
        : Probability Rules:
            : Negation
            : Inclusion-Exclusion
            : Marginalization
            : Conditioning
        : Inference by Enumeration
        : Approximate Inference:
        : Sampling
            : Rejection Sampling
            : Likelihood Weighting
        : hidden Markov Model: Task:
            : Filtering = given objservations from start until now, calculate distribution for current state
            : prediction = given observations from start until now, calculate distribution for a future state
            : smoothing = given observations from start until now, calculate distribution for past state
            : most likely explanation = given observatinos from start until now, calculate most likely sequence of states
    : OPTIMIZATION:
        : Local Search = search algorithms that maintain a single node and searches by moving to a neighboring node
            : Hill Climbing
                : function HILL-CLIMB(problem):
                    : current = initial state of problem
                    : repeat:
                        neighbor = highest valued neighbor of current
                        if neighbor not better than current:
                            return current
                        current = neighbor
                : steepest-ascene = choose the highest-valued neighbor
                : stochastic = choose randomly from higher-valued neighbors
                : first-choice = choose the first higher-valued neighbor
                : random-restart = conduct hill climbing multiple times
                : local beam search = chooses the k highest-valued neighbors
            : Simulated Annealing =
                function SIMULATED-ANNEALING(problem, max):
                    current = initiate state of problem 
                    for t = 1 to max:
                        T = TEMPERATURE(t)
                        neighbor = random neighbor of current 
                        deltaE = how much better neigh is than current
                        if deltaE > 0:
                            current = neighbor
                        with probability e^deltaE/T set current = neighbor
                    return current
            : Linear Programming:
                : Simplex
                : Interior-Point
            : arc consistency = when all the values in a variable's domain satisfy the variable's binary constraints
                : function REVISE(csp, X, Y):
                    revised = false
                    for x in X.domain:
                        if no y in Y.domain satisfies constrain for (x, Y):
                            delete x from X.domain
                            revised = true
                        return revised
                : function AC-3(csp)
                    queue = all arcs in csp
                    while queue non-empty:
                        (X, Y) = DEQUEUE(queue)
                        if REVISE(csp, X, Y):
                            if size of X.domain == 0:
                                return false
                            for each Z in X.neighbors - {Y}:
                                ENQUEUE(queue, (Z, X))
                        return true 
            : Backtracking Search:
                : function BACKTRACKING(assignment, csp):
                    if assignment complete: return assignment
                    var = SELECT-UNASSIGNED-VAR(assignment, csp)
                    for value in DOMAIN-VALUES(var, assignment, csp):
                        if value consistent with assignment:
                            add {var = value} to assignment
                            result = BACKTRACK(assignment, csp)
                            if result not= failure: return result
                        remove {var = value} from assignment
                    return failure
            : maintaining arc consistency = algorithm for enforcing arc-consistency every time we make a new assignment 
                : function BACKTRACKING(assignment, csp):
                    if assignment complete: return assignment
                    var = SELECT-UNASSIGNED-VAR(assignment, csp)
                    for value in DOMAIN-VALUES(var, assignment, csp):
                        if value consistent with assignment:
                            add {var = value} to assignment
                            inferences = INFERENCES(assignment, csp)
                            if inferences not= failure: add inferences to assignment
                            result = BACKTRACK(assignment, csp)
                            if result not= failure: return result
                        remove {var = value} and inferences from assignment
                    return failure

    : ARITHMETIC: XOR (HALF-ADDER), FULL-MULTI-BIT ADDER
    : COMPARE: =, <, >
    : LIST: QUEUE, STACK
    : LEARNING:
        : supervised learning = given a data set of input-output pairs, learn a function to map inputs to outputs
            : classification = supervised learning task of learning a function mapping an input point to a discrete category
                : nearest-neighbor classification = algorithm that, given an input, chooses the class of the nearest data point to that input
                : k-nearest-neighbor classification = algorithm that, given an input, chooses the most common class out of the k nearest data points to that input
            : hypothesis hw(x) = 1 if w * x > 0; 0 otherwise
            : perceptron learning rule = given data point (x, y), update each weight according to: 
                : wi = wi + a(y - hw(x)) * xi
                : wi = wi + a(actual value - estimate) * xi
                : Only capable of learning linearly separable decision boundary.
            : Support Vector Machine
                : boundary that maximizes the distance between any of the data points 
            : regression = supervised learning task of learning a function mapping an input point to a continuous value
            : regularization = penalizing hypotheses that are more complex to favor simpler, more general hypotheses
            : holdout cross-validation = splitting data into a training set and a test set, such that learning happens on the training set and is evaluated on the test set
            : k-fold cross-validation = splitting data into k sets, and experimenting k times, using each set as a test set once, and using remaining data as training set
        : reinforcement learning = given a set of rewards or punishments, learn what actions to take in the future
            : Q-learning = method for learning a function Q(s,a), estimate of the value of performing action a in state s
                : Start with Q(s,a)=0 for all s,a
                : When we take an action and receive a reward:
                    : Estimate the value of Q(s,a) based on current reward and expected future rewards
                    : Update Q(s,a) to take into account old estimate as well as our new estimate
            : Greedy Decision-Making
                : When in state s, choose action a with the highest Q(s,a)
            : Explore vs Exploit 
                : Epsilon-Greedy:
                    : Set epsilon equal to how often we want to move randomly.
                    : With probability 1 - epsilon, choose estimated best move.
                    : With probability epsilon, choose random move.
            : function approximation = approximating Q(s,a) often by a function combining various features, rather than storing one value for every state-action pair
        : unsupervised learning = given input data without any additional feedback, learn patterns
            : clustering = organizing a set of objects into groups in such a way that similar objects tend to be in the same group
                : k-means clustering = algorithm for clustering data based on repeatedly assigning points to clusters and updating those clusters' centers
                :  
        : Neural Networks
            : Neurons are connected to and receive electrical signals from other neurons.
            : Neurons process input signals and can be activated.
            : artificial neural network = mathematical model for learning inspired by biological neural networks
                : Model mathematical function from inputs to outputs based on the structure and parameteres of the network.
                : Allows for learning the network's parameters based on data.
                : Functions:
                    : step function: g(x) = 1 if x > 0, else 0
                    : logic sigmoid
                    : rectified linear unit (relu)
                : h(x1, x2) = g(w0 + w1x1 + w2x2)
                : h(x1, x2, xn-1, xn) = g(n sigma i=1 xiwi + w0)
            : supervised neural network: 
                : gradient descent = algorithm for minimizing loss when training neural network
                    : Start with a random choice of weights.
                    : Repeat: 
                        : Calculate the gradient based on ALL DATA POINTS: direction that will lead to decreasing loss.
                        : Update weights according to the gradient.
                : Stochastic gradient descent
                    : Start with a random choice of weights.
                    : Repeat: 
                        : Calculate the gradient based on ONE DATA POINTS: direction that will lead to decreasing loss.
                        : Update weights according to the gradient.
                : Mini-Batch gradient descent
                    : Start with a random choice of weights.
                    : Repeat: 
                        : Calculate the gradient based on ONE SMALL BATCH: direction that will lead to decreasing loss.
                        : Update weights according to the gradient.
            : multilayer neural network = artificial neural network with an input layer, an output layer, and at least one hidden layer 
            : backpropagation = algorithm for training neural netowrks with hidden layers
                : Start with a random choice of weights.
                : Repeat:
                    : Calculate error for output layer.
                    : For each layer, starting with output layer, and moving inwards towards earliest hidden layer:
                        : Propagate error back one layer.
                        : Update weights.
            : deep neural networks = neural network with multiple hidden layers
            : dropout = temporarily removing units - selected at random - from a neural network to prevent over-reliance on certain units
            : computer vision = computational methods for analyzing and understanding digital images
            : image convolution = applying a kernel/filter that adds each pixel value of an image to its neighbors, weighted according to a kernel matrix
            : pooling = reducing the size of an input by sampling from regions in the input
                : max-pooling = pooling by choosing the maximum value in each region
            : convolutional neural network = neural networks that use convolution, usually for analyzing images
                : Apply convolution
                : Apply pooling
                : Apply Flattening
                : Apply Deep Neural Networks
            : feed-forward neural network = neural network that has connections only in one direction
                : input >>> network >>> output
            : recurrent neural network: input >>> network (loop) >>> output
                : Relationships:
                    : one-to-many 
                    : many-to-one
                    : many-to-many
        : Natural Language Processing:
            : tokenization = the task of splitting a sequence of characters into pieces (tokens)
            : text classification =
            : Naive-Bayes classifier = 
            : additive smoothing = adding a value a to each value in our distribution to smooth the data  
                : Laplace smoothing = adding 1 to each value in ou r distribution: pretending we've seen each value one more time than we actually have
            : Word Representation
            : Question "What is that?" >>> Recurrent Network >>> Answer "This is apple."
            : Encoder >>> Hidden State >>> Decoder
            : Attention = letting us decide which values are important to pay attention to when generating, in this case, the next word in our sequence.
            : Machine Translation 
            : Transformers Architecture
                : Encoder: input word + positional encoding >>> (multi-head self attention >>> neural network) * Number >>> encoded representation
                : Decoder: previous output word + positional encoding >>> (multi-head self attention >>> (encoded representations) attention >>> neural network) * Number >>> encoded representation
