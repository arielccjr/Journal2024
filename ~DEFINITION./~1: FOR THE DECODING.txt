~COMPUTER-SCIENCE: REASONING
    ~1: SEARCH/Queries: Retrieves specific data from the database. To access and analyze stored information.
        ~1: Define Initial State: Start with a known state.
        ~2: Check Goal State: Verify if the current state meets the goal criteria.
        ~3: Expand Nodes: Generate possible next states from the current state.
        ~4: Store in Frontier: Maintain a list of unexplored nodes.
        ~5: Use Search Strategy: Choose the next state based on the algorithm.
        ~6: Avoid Infinite Loops: Track visited states to prevent redundant exploration.
        ~7: Find Optimal Solution: Minimize path cost if applicable.
            : DEPTH-FIRST SEARCH:
                ~1: Push Initial State into Stack Frontier.
                ~2: Loop until Solution or Empty Frontier:
                    : Pop the last added node.
                    : If goal state, return solution.
                    : Otherwise, expand node and push new nodes into the stack.
                ~3: Backtrack when dead ends are reached.
            : BREADTH-FIRST SEARCH:
                ~1: Enqueue Initial State into Queue Frontier.
                ~2: Loop until Solution or Empty Frontier:
                    : Dequeue the first added node.
                    : If goal state, return solution.
                    : Otherwise, expand node and enqueue new nodes.
            : A* SEARCH: 
                ~1: Initialize Priority Queue Frontier with Initial State.
                ~2: Loop until Solution or Empty Frontier:
                    : Remove the node with the lowest f(n) = g(n) + h(n).
                    : If goal state, return solution.
                    : Otherwise, expand node and add new nodes with updated f(n).    
            : MINIMAX ALGORITHM (For Adversarial Search) 
                ~1: Recursively Explore Possible Moves.
                ~2: If Terminal State, Return Utility.
                ~3: If Max Player, Choose Move with Maximum Utility.
                ~4: If Min Player, Choose Move with Minimum Utility. 
    ~2: KNOWLEDGE:
        : INFERENCE BY RESOLUTION:
            ~1: Convert all knowledge into Conjunctive Normal Form (CNF):
                : Remove biconditionals (↔) and implications (→).
                : Apply De Morgan’s Laws to push NOT operators inward.
                : Distribute OR over AND to standardize logical clauses.
            ~2: Assume the negation of the query (¬Q).
            ~3: Use Resolution Inference:
                : Find clauses with complementary literals (P and ¬P).
                : Resolve them to create new clauses.
                : Repeat until empty clause (∅) is derived (indicating a contradiction).
            ~4: If an empty clause is found, the query is true; otherwise, it's false.
    ~3: CERTAINTY: inability of artificial intelligence to have perfect knowledge about the world, requiring it to operate based on probabilities rather than certainties
        : Conditional Probability Calculation: To determine P(A | B) (the probability of A given B), we use the formula:
            P(A∣B)=P(A∩B)P(B)
            P(A∣B)=P(B)P(A∩B)
        : Sampling for Approximate Inference: When exact calculations are computationally expensive, we can approximate probabilities by generating many random samples.
        : Bayes' Rule: P(b | a) = P(a | b) P(b) / P(a)
        : Probability Rules:
            : Negation
            : Inclusion-Exclusion
            : Marginalization
            : Conditioning
        : Inference by Enumeration
        : Approximate Inference:
        : Sampling
            : Rejection Sampling
            : Likelihood Weighting
        : hidden Markov Model: Task:
            : Filtering = given objservations from start until now, calculate distribution for current state
            : prediction = given observations from start until now, calculate distribution for a future state
            : smoothing = given observations from start until now, calculate distribution for past state
            : most likely explanation = given observatinos from start until now, calculate most likely sequence of states
    ~4: OPTIMIZATION: the process of selecting the best option from a set of possible choices to achieve a specific goal efficiently.
        : Hill Climbing: algorithm follows a simple logic:
            ~1: Start with an initial state.
            ~2: Evaluate neighboring states.
            ~3: Move to the best neighbor if it improves the solution.
            ~4: Repeat until no better neighbor exists.
        : Simulated Annealing: introduces randomness in the search process by sometimes accepting worse states, with decreasing probability over time.
        : Linear Programming is used for problems where constraints must be satisfied while minimizing or maximizing a cost function.
    ~5: LEARNING: 
        : Supervised learning = given a data set of input-output pairs, learn a function to map inputs to outputs
            ~1: Prepare Data: Collect labeled input-output pairs.
            ~2: Split Data: Divide into training and testing sets.
            ~3: Train Model: Fit a hypothesis function to the training data.
            ~4: Evaluate Model: Test the trained model on unseen data.
            ~5: Adjust Weights: Update parameters based on accuracy.
            : reinforcement learning = given a set of rewards or punishments, learn what actions to take in the future
            : unsupervised learning = given input data without any additional feedback, learn patterns
        : Nearest Neighbor Classification: A simple supervised learning method where classification is based on the closest labeled data points: 
            ~1: Store all training data.
            ~2: For a new input:
                : Compute distance to all stored points.
                : Select the closest k neighbors.
                : Assign the most common class among neighbors.
            ~3: Output the predicted class.
        : Perceptron Learning: A linear model that adjusts weights iteratively based on errors in classification:
            ~1: Initialize weights randomly.
            ~2: For each data point (x, y):
                : Compute prediction using dot product.
                : Compare prediction with actual output.
                : If incorrect, adjust weights: wi=wi+α(y−y^)xi
            ~3: Repeat until convergence.
    ~6: NEURAL NETWORKS:
        ~1: Define Inputs & Outputs: "We begin with defining the input variables x1,x2,...xnx1,x2,...xn and setting up the desired outputs." (Neural Networks, p. 3).
        ~2: Assign Weights & Biases: "Each input is multiplied by a corresponding weight, with an additional bias term." (Neural Networks, p. 4).
        ~3: Apply Activation Function: "An activation function is applied to determine when a neuron becomes active." (Neural Networks, p. 6).
        ~4: Calculate Output: "The network then processes the weighted sum of inputs and passes it through layers to get the final output." (Neural Networks, p. 7).
        ~5: Train the Network Using Backpropagation: "Gradient descent is used to adjust the weights to minimize the loss function." (Neural Networks, p. 15).
 
~NATURAL LANGUAGE: TRANSFORMERS ARCHITECTURE:
    : Encoder: input word + positional encoding >>> (multi-head self attention >>> neural network) * Number >>> encoded representation
    : Decoder: previous output word + positional encoding >>> (multi-head self attention >>> (encoded representations) attention >>> neural network) * Number >>> encoded representation
    ~1: Encoder-Decoder Architecture
        : Encoder: Processes the input sentence into a fixed representation.
        : Decoder: Generates the output sequence, one word at a time.
    ~2: Self-Attention Mechanism
        Instead of using RNNs, the Transformer calculates attention scores for all words in a sentence simultaneously.
        Multi-Head Attention enables different aspects of word relationships to be captured.
    ~3: Positional Encoding
        Since Transformers do not process sequences sequentially, positional encoding is added to retain information about word order.
    ~4: Feed-Forward Layers
        Each word's representation is passed through fully connected layers to extract deeper features.
    ~5: Masked Self-Attention in the Decoder
        Prevents words from attending to future words (maintains causality).
    ~6: Output Prediction
        The decoder predicts the next word using a softmax function, generating the final output.
    ~7: Optimization and Training
        Uses the Adam optimizer with learning rate scheduling.
        Dropout and Label Smoothing are applied for regularization.
        Beam Search is used for better sentence generation.

~ARISTOTLE'S THE ORGANON: DEDUCTIVE REASONING
    : CATEGORIES: classifies different kinds of things that can be said about a subject.
        ~1: Identify the Subject, the Primary substance, that is the individual entities (e.g., Socrates, this particular tree).
        ~2: Determine the Subject’s Substance (substantiam, οὐσία), that refers to the core identity or essence of things, answering "What?"
        ~3: Analyze its Attributes, Accidental properties, the Characteristics that vary without changing substance, Using the Remaining Nine Categories
            : Quantity – How much or how big something is (e.g., "five feet tall," "three liters").
            : Quality – What kind of thing it is (e.g., "red," "brave," "intelligent").
            : Relation – How it is related to something else (e.g., "bigger than," "father of").
            : Place – Where it is (e.g., "in the room," "at the market").
            : Time – When it is (e.g., "yesterday," "at noon").
            : Position – How it is arranged (e.g., "sitting," "lying down").
            : State/Condition – What condition it is in (e.g., "armed," "healthy," "wet").
            : Action – What it is doing (e.g., "running," "writing").
            : Passion (Being Acted Upon) – What is being done to it (e.g., "being burned," "being pushed").
    : ON INTERPRETATION (Peri Hermeneias): is a key work in his Organon, dealing with logic, language, and meaning. It focuses on how statements (propositions) express truth and falsity and lays the foundation for modal logic, contradiction, and necessity.
        ~1: Words & Meaning:
            : Words are symbols of mental concepts.
            : Propositions (statements) are combinations of words that express truth or falsity.
            : Example:
                "The sky is blue." → A statement that can be true or false.
                "Walk quickly!" → Not a statement (it’s a command).
        ~2: Affirmation & Negation: A proposition and its negation cannot both be true at the same time (Law of Non-Contradiction).
            : Every proposition affirms or denies something.
            : Affirmation: “Socrates is wise.”
            : Negation: “Socrates is not wise.”
        ~3: Contradictions & Opposites (Square of Opposition)
            : Contradictory statements: One must be true, the other false. “It is day.” vs. “It is not day.”
            : Contrary statements: Both cannot be true, but both can be false. “All men are wise.” vs. “No man is wise.”
        ~4: Modal Logic (Necessity vs. Possibility): Implication: Modal logic helps in predicting future truths without contradiction.
            : Necessity: Something must be true. “The sun must rise.”
            : Possibility: Something may or may not be true. “It may rain tomorrow.”
    : PRIOR ANALYTICS: explains how conclusions necessarily follow from given premises when structured correctly.
        ~1: Defining the Syllogism: A syllogism is a logical argument where a conclusion follows necessarily from two premises. It has three parts:
            : Term 1 (Major term) – Found in the major premise and conclusion. "All mammals (Middle Term) are warm-blooded. (Major Premise)"
            : Term 2 (Minor term) – Found in the minor premise and conclusion. "Whales are mammals. (Minor Premise)"
            : Term 3 (Middle term) – Links the two premises but does not appear in the conclusion. "Therefore, whales are warm-blooded. (Conclusion)"
        ~2: Identifying Syllogistic Figures & Moods: Aristotle categorized syllogisms into different figures and moods based on how terms are positioned.
            : Figure 1: Middle term is subject in one premise, predicate in another.
            : Figure 2: Middle term is predicate in both premises.
            : Figure 3: Middle term is subject in both premises.
            : Figure 4: Middle term switches placement.
            Each figure contains valid or invalid moods, which define how premises are arranged using:
                : A (Universal Affirmative: "All A are B")
                : E (Universal Negative: "No A are B")
                : I (Particular Affirmative: "Some A are B")
                : O (Particular Negative: "Some A are not B")
        ~3: Distinguishing Between Valid & Invalid Reasoning
            : Valid syllogisms: The conclusion follows necessarily.
            : Invalid syllogisms: A logical error occurs (e.g., fallacies, undistributed middle).
            : Example of an invalid syllogism (Undistributed Middle Fallacy):
                All dogs are animals.
                All cats are animals.
                Therefore, all dogs are cats. ❌ (Incorrect reasoning)
        ~4: Using Reduction & Proofs: Aristotle reduced complex arguments to basic valid forms using:
            : Direct reduction: Rearranging premises to match known valid syllogisms.
            : Indirect proof (reductio ad absurdum) to disprove invalid reasoning.   
    : POSTERIOR ANALYTICS: develops a theory of scientific knowledge (epistēmē) to explain how demonstrative knowledge (apodeixis) is structured.
        ~1: The Definition of Demonstrative Knowledge: Aristotle begins by defining scientific knowledge:
            : Knowledge (epistēmē) must be certain and universal.
            : It must be acquired through demonstration (apodeixis), a syllogism whose premises are true, necessary, and primary.
            : The premises themselves must be better known than the conclusion.
            Example of Demonstration:
                All humans are mortal. (Universal and necessary premise)
                Socrates is a human. (Particular known fact)
                Therefore, Socrates is mortal. (Demonstrated conclusion)
        ~2: The Role of First Principles (Axioms)
            : Every science must begin with self-evident first principles (archai).
            : These principles cannot themselves be demonstrated but are known through intellectual intuition (nous).
            : They must be:
                : True (not subject to doubt).
                : Primary (not derived from anything else).
                : Immediate (not requiring proof).
                ; More knowable than conclusions.
            Example in Geometry:
                Axioms like "The whole is greater than the part" must be accepted without proof.
                From such axioms, we derive theorems in Euclidean geometry.
        ~3: The Process of Scientific Discovery
            : Induction (Epagōgē): We observe patterns in nature.
                Example: Seeing multiple instances of fire being hot.
            : Recognition of First Principles (Nous): Through repeated observation, the intellect grasps universal truths.
                Example: Realizing "Fire is always hot" is a universal principle.
            : Deduction (Syllogismos): From these first principles, we derive further necessary truths using logical reasoning.
                Example: Fire causes heat → Heat expands metals → Fire expands metals.
        ~4: The Hierarchical Structure of Knowledge: Aristotle emphasizes that sciences form a hierarchy:
            : Some sciences, like mathematics, rely only on internal logical structures.
            : Others, like natural science, depend on observation and induction.
            : The most fundamental knowledge is metaphysics, which deals with being and first principles themselves.       
    : TOPICS: he defines as the art of constructing and evaluating arguments based on commonly accepted opinions (endoxa), focusing on how one can argue both for and against a proposition.
        ~1: Identifying the Subject Matter
            : Define the topic of discussion and categorize it properly.
            : Recognize the general premises accepted about the subject.
        ~2: Finding a Commonplace (Topos) for Argument
            : Use topoi (commonplaces) to structure the argument.
            : Topoi are general patterns of reasoning that can be applied to different subjects.
        ~3: Forming Premises from Endoxa (Common Opinions)
            : Identify widely accepted opinions (from experts, common people, or reputable sources).
            : Ensure that the premises align with probable truth rather than mere assumption.
        ~4: Constructing Arguments (Syllogistic or Inductive)
            : Use deductive reasoning (syllogism) → Argument moves from general to specific.
                E.g., "All virtue is praiseworthy; courage is a virtue; therefore, courage is praiseworthy."
            : Use inductive reasoning (examples & analogies) → Argument moves from specific to general.
                E.g., "Socrates and Plato were wise because they pursued knowledge; therefore, philosophers are wise."
        ~5: Examining and Refuting Opposing Arguments
            : Identify weaknesses in opposing premises (ambiguities, contradictions, false assumptions).
            : Apply counter-topoi (counterarguments) to refute opposing claims.
        ~6: Strengthening One’s Own Argument
            : Clarify definitions and avoid logical fallacies.
            : Use analogies and illustrative examples to make the argument stronger.
    : SOPHISTICAL ELENCHI: dedicated to the study of fallacious reasoning and false argumentation, primarily focusing on eristic (contentious) arguments used in debate 
        ~1: Identify the Fallacy Type – Determine whether the error is linguistic or conceptual.
            : For fallacies in language, clarifying ambiguous terms, sentence structure, or assumptions usually suffices. 
                : Equivocation: Using a word with multiple meanings ambiguously in an argument.
                    If an argument says, “The law should be followed because it is the ‘law of nature,’” 
                    clarify that legal law and natural law are distinct concepts, thus invalidating the argument’s logic.
                : Amphiboly: Ambiguity arising from poor sentence structure or unclear phrasing.
                    If someone says, “Flying planes can be dangerous,” 
                    clarify whether they mean that piloting planes is dangerous or that planes in flight are hazardous.
                : Accent: Changing the meaning of a statement by emphasizing different words.
                    For “I didn’t say she stole the money,” 
                    show how different emphases change the implied meaning and that no conclusion can be reliably drawn.
                : Composition: Assuming what is true of parts is true for the whole.
                    If someone argues, “All parts of the car are lightweight, so the whole car must be lightweight,” 
                    show that a car’s total weight differs from its individual parts.
                : Division: Assuming what is true of the whole is true for each part.
                    If an argument states, “The team is successful, so every team member is successful,” 
                    point out that team success doesn’t imply each member’s personal success.
            : Fallacies Outside Language: focusing on the logical structure, causation, or evidentiary support is essential. 
                : Begging the Question (Petitio Principii): Assuming the truth of what the argument is supposed to prove.
                   For “Reading is beneficial because it’s good for you,” 
                    explain that “good for you” is simply restating “beneficial” without further evidence.
                : False Cause (Post Hoc Ergo Propter Hoc): Assuming that because one event follows another, the first caused the second.
                    For “I started wearing my lucky charm, and my grades improved,” 
                    explain that improved grades may be due to other factors, like studying harder.
                : Accident: Misapplying a general rule to an exception or specific case.
                    For “Cutting people is wrong, so surgeons are immoral,” 
                    clarify that surgery is a controlled, beneficial exception to the general rule against cutting.
                : Converse Accident (Hasty Generalization): Drawing a broad conclusion from a limited sample.
                    For “One politician lied, so all politicians are liars,” 
                    explain that one instance doesn’t justify generalizing about all politicians.
                : False Analogy: Comparing two things that aren’t sufficiently similar in relevant aspects.
                    If someone argues, “Employees are like nails; you have to hit them on the head to make them work,” 
                    explain that employees and nails are different and shouldn’t be treated the same way.
                : Slippery Slope: Assuming a small first step will lead to a chain of extreme events.
                    For “If we let students redo assignments, they’ll expect to retake exams,” 
                    explain that allowing assignment revisions doesn’t logically lead to redoing all assessments.
                : Ad Hominem: Attacking the person instead of the argument itself.
                    If someone argues, “You shouldn’t listen to her because she’s not a scientist,” 
                    clarify that the person’s background doesn’t inherently disprove their argument’s validity.
                : Appeal to Authority (Ad Verecundiam): Relying on an authority figure outside their area of expertise.
                    If a celebrity endorses a health product, 
                    explain that fame does not qualify them as a medical expert, and scientific evidence is required for credibility.
        ~2: Break Down the Argument – Analyze the terms, premises, and conclusion.
        ~3: Clarify Ambiguities – If the argument relies on wordplay, expose the different meanings.
        ~4: Challenge the Premises – If an assumption is false or weak, point it out.
        ~5: Show the Contradiction – Demonstrate how the fallacy leads to absurdity or contradiction.
 
~ARCHETYPES (TOPICS):
    -1: "SLEEPING FOR-BY DREAM." "WHAT IF... ? YOU ARE RIGHT!: MADNESS, NIGHTMARE, Worst Case, FALLACIES: DIVISION, LACKING
        "SPOIL FOR-BY SOILVOID(-)." : Disruptions, Damages, Uncontrolled Elements (Dice), Expenses, taxes, and liabilities, Low Health, Resource Scarcity/Finity, Bankruptcy
        "BAIT/SACRIFICE FOR-BY MOTHERBEAST.": NPCs, Requests/Puzzles, Opposition, Alone, Low Population: Wild Encounters + Enemy Raids/Ambush, Rivals, NPCs, Trainers, Gyms, Elite Four, and the Champion
        "TRAP FOR-BY WILDWORLD.": The Rat Race, 
        "WAIT FOR-BY BIRTHSTART.": Initial State, Beginning Again
    +1: "AWAKENING FOR-BY REALITY." "I KNOW! THAT'S RIGHT!": REASON, LOGIC, REALIZATION OF THE DREAM, ONENESS, COMPLETENESS/FULFILLMENT!
            : Defuse or trigger the bomb.
            : Win Condition: To win the game, players must fulfill their dream on the Fast Track or reach a certain level of passive income.
            : Winning rounds builds your team’s momentum, giving you both an economic and psychological edge in the match.
        "FRUIT FOR-BY SEEDWORD(+)."
            : Mental Dictionary/Library, pokedex, Map, Contact List,
            : Opportunities: Resources: Money, Income, Investments, Passive Income, Items, Badges/Priviledges
            : Art & Design: Color, Logo, Web, Comics, Video, Audio, Game, Store Set Up
            : Experience Points, Abilities/Stats, New Moves, Karma, 
        "SON FOR-BY FATHERGOD.": Team: Accountant, Investors, Publishers, Clients: Aristocrats, Professionals, Fathers, Fraternity and Brotherhoods, Gentlemen, Nomads, the “New Rich”
            : Club Executives:
            : Testimonials: Reviews
        "WAY FOR-BY KINGDOMHEAVEN."
            : Creative Directing Studio: Channels/Web/Store: creativedirecting.studio, Phone: 780-996-5088, Email: arielccjr@creativedirecting.studio
            : Build Structures/LEVELS: Balance scope budget, and schedule  
                : NAVIGATION, Maps, Atmosphere, Weather,
                : Progressing the game storyline and accessing new areas.
                : better positioning on the map, better territory
        "CHANCE FOR-BY DEATHEND."
            : Timeline: Every time the player passes "Payday" on the board, they receive their paycheck.
            : Incorporation/Fiscal Year: December 1, 2024... 1-2 days per project
            : PROGRESS = Tutorials, Saves, Updates: Journal?
            : Battle Cycle: 1 move/turn at a time: Best of 7 Rounds: Round-Time: 3 minutes
            : DEADLINE: Goal State, Completion
            : Fast Track, Here, they focus on making larger investments and fulfilling their dream, which is a personal goal they select at the start of the game.
